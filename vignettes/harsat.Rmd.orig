---
title: "Getting started"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Getting started

For now, we will assume that you are familiar with running basic R scripts
and so on. We will focus on what the structure of a typical what the code 
looks like in a typical analysis.

## Installing `harsat` from GitHub

To install the latest development version, use the `remotes` package:

```
library(remotes)
remotes::install_github("osparcomm/harsat", auth_token = 'XXXX') 
```

The stable version is similar:

``` r
# install.packages("devtools")
devtools::install_github("osparcomm/HARSAT@main")
```

> Note: during development the repository is marked as private on GitHub, so you
> will need a Personal Access Token (or PAT) to access it. Follow
> [these instructions to create a Personal Access Token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-fine-grained-personal-access-token).
> It'll be a short string, probably beginning with `ghp_`. Put the whole
> string into the `auth_token` parameter, and that should install the 
> `harsat` package directly.

> Note: many of the functions currently have a `cstm` prefix. When the
> code was originally developed, we thought of it as "contaminant time series modelling", 
> which is why you get all these `cstm` prefixes. These will be removed in the 
> near future.

## Loading the code

Now, within R, you can load the library in the usual way

```{r}
library(harsat)
```

## Accessing files

How you organize your files is up to you. We will start from the current working directory.
We'll detect that using R's `here` package, and write it into a variable `working.directory`. 

```{r}
library(here)
working.directory <- here()
```


## Reading in the data

The first step is to read in the data that we've got.
We will go through some of these arguments.

```{r getting-started-read}
biota_data <- read_data(
  compartment = "biota", 
  purpose = "OSPAR",                               
  contaminants = "test_data.csv", 
  stations = "station_dictionary.csv", 
  QA = "quality_assurance.csv",
  data_dir = file.path(working.directory, "data", "example_simple_OSPAR"),
  data_format = "ICES_old",
  info_files = list(
    determinand = "determinand_simple_OSPAR.csv", 
    thresholds = "thresholds_biota_simple_OSPAR.csv"
  ),
  info_dir = file.path(working.directory, "information"), 
  extraction = "2022/01/11",
  max_year = 2020L
)
```

The main arguments here are as follows:

* `compartment` is an argument which specifies whether we're dealing with a 
  biota assessment, a sediment assessment, or a water assessment.
* `purpose` means we can mirror an `OSPAR` style assessment, a `HELCOM` style assessment, 
  an `AMAP`-style assessment, or `other` which means you can basically tailor it yourself 
  and the idea is that the code will be sufficiently flexible that you can do a lot of 
  tailoring to suit your own needs.
* `contaminants` is a data file which has all the chemical measurements in it.
* `stations` is the station file which is directly related to the station dictionary 
  that we get out at from ICES.
* `QA` is a quality assurance file -- this is *deprecated* and will disappear.
* `info_files` are reference tables -- we come back to that.

This reads in the three data sets, but does no more than that at this stage.

Once you get to this point, you can look at the data if you want to, or 
do anything else that you need with the data before proceeding. 
Essentially the files that come in at this point are unchanged from the files that we are reading them in from.
This is basically just reading in the data and setting things up.

At this point we might want to do a whole lot of *ad hoc* corrections to these data,
as is done with the OSPAR estimates. 

## Tidying the data

The next step is to clean the data to prepare for analysis. This step tidies up the 
data structures that we've got there. It does filtering so that we get data in the
 form that we want for say, an OSPAR assessment or a HELCOM assessment.
It also streamlines some of the data files.

This may generate warnings. For example, when we're cleaning the station dictionary, 
we've may find some issues with duplicate stations. Similarly, when it comes to cleaning the contaminant and biological effects data, 
we've may find some data from stations which are unrecognized by the station dictionary. Sometimes that's fine and sometimes that's not.
Warnings are supported by output files which allow you to come in and have a look and 
see which values are affected, so you can go and check them out in more detail.

```{r getting-started-tidy}
biota_data <- ctsm_tidy_data(biota_data)
```

To this point, we've still not done anything with the datasets.

## Create time series

Now we can group the data into time series. This means identifying which data points 
belong to the same time series, and then set it up as a structure which then allows us 
to do the assessments.

There are various different ways in which we can specify which determinants we actually 
want to assess. In this case, the `determinands` parameter specifies cadmium, CP153, CB153, HBCD, HBCDA,
HBCDG, and some biological metabolites. There are various other arguments which allow you to control 
just how the data are manipulated.

```{r getting-started-time-series}
biota_timeSeries <- ctsm_create_timeSeries(
  biota_data,
  determinands = c("CD", "CB153", "HBCD","HBCDA", "HBCDG", "PYR1OH"),
  determinands.control = list(
    HBCD = list(det = c("HBCDA", "HBCDB", "HBCDG"), action = "sum"),
    "LIPIDWT%" = list(det = c("EXLIP%", "FATWT%"), action = "bespoke")
  ),
  get_basis = get_basis_biota_OSPAR
)
```

## Assessment

The next the next stage is to do the assessment.

We've created a time series object, and pass that into this call.
We have to say which thresholds we're going to use when we do the assessment and there are other options which we can put in here.
But this will just run.
You can see that it tells you which time series we're actually assessing as it progresses.
This gives you an idea of how how many cups of tea you can drink before it's all finished.

```{r getting-started-assessment}
biota_assessment <- run_assessment(
  biota_timeSeries,
  AC = c("BAC", "EAC", "EQS", "HQS")
)
```

There are various little warnings: these ones are nothing to worry about.

We can then check that everything is converged.

```{r getting-started-convergence}
check_convergence_lmm(biota_assessment$assessment)
```

## Reporting

We can then get a summary table of our results. We want a direcrory where
we can put it. And `harsat` won't create a directory if there's nothing
there, so let's make a new directory, `./output/tutorial`, and put the full
path into `summary.dir`, so we can tell `harsat` where to write to.

```{r getting-started-summary-dir}
summary.dir <- file.path(working.directory, "output", "tutorial")

if (!dir.exists(summary.dir)) {
  dir.create(summary.dir, recursive = TRUE)
}
```

Next, we set up a few variables to manage the display.

```{r getting-started-summary-1}
webGroups <- list(
  levels = c("Metals", "Metabolites", "Organobromines", "Chlorobiphenyls"),  
  labels = c(
    "Metals", "PAH metabolites", "Organobromines",  "Polychlorinated biphenyls"
  )
)

classColour <- list(
  below = c(
    "BAC" = "blue", 
    "EAC" = "green", 
    "EQS" = "green",
    "HQS" = "green"
  ),
  above = c(
    "BAC" = "orange", 
    "EAC" = "red", 
    "EQS" = "red",
    "HQS" = "red"
  ), 
  none = "black"
)
```

And then we can generate the summary proper.
The summary file which will be very familiar to those who have been involved in the OSPAR and the HELCOM
assessments. The summary files have information about each time series, 
what the time series represents (which is the first set of columns), followed
by statistical results, such as p values, and summary values such as the number of years in the data set,
when it starts and finishes. And towards the end, we've got comparisons against various different threshold values.

```{r getting-started-summary-2}
write_summary_table(
  biota_assessment, 
  determinandGroups = webGroups,
  classColour = classColour,
  collapse_AC = list(EAC = c("EAC", "EQS")),
  output_dir = summary.dir 
)
```

